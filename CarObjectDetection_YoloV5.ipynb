{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection on Vehicle Dataset Images"
      ],
      "metadata": {
        "id": "cLiHXDiot_xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWojw42WjBcR",
        "outputId": "dcd9c28d-465a-4623-dd42-f89b3a443be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch implementation of YoloV5 object detection\n",
        "* Installations and Imports\n",
        "* Dataset Loading\n",
        "* Model Loading\n",
        "* Training model\n",
        "* Evaluation\n",
        "* Inference\n"
      ],
      "metadata": {
        "id": "1KXLImQIuBFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Necessary Modules"
      ],
      "metadata": {
        "id": "qSymWekouPLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/SEM 1/TML/yolov5\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ab8MeLuXuO",
        "outputId": "f02378f9-71de-4d5c-db9d-b336eee375c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1LqhIB71FrBRNw-WAPjOHPWbbEnFt7cvQ/SEM 1/TML/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8K9WKgiudaM",
        "outputId": "6d56fa8e-ba5b-458a-b4b4-99af56e233f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone"
      ],
      "metadata": {
        "id": "51dH2B64ujNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPNAeqeYuj_e",
        "outputId": "de0c8786-de3f-4db0-90f0-bafb86d38f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ 2023-11-25 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.9/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHoOZ467hW59"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import urllib\n",
        "import PIL.Image as Image\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "rcParams['figure.figsize'] = 16, 10\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Collection\n",
        "We have used Vehicle Dataset by  Yudha Bhakti Nugraha and Kris - https://universe.roboflow.com/roboflow-100/vehicles-q0x2v\n",
        "\n",
        "It contains the data of around 4-5k images, divided into train, val and test set. With a json file for each containing image information like bounding boxes and classes."
      ],
      "metadata": {
        "id": "aqrfsifPvET1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data convert to yolo format annotation\n",
        "\n",
        "### Each image have a txt file containing the information in following order - class_id, x_centre, y_centre, width, height"
      ],
      "metadata": {
        "id": "NWGTxpFWvIQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "count_dict = {}\n",
        "class COCO2YOLO:\n",
        "    def __init__(self, json_file, output):\n",
        "        self._check_file_and_dir(json_file, output)\n",
        "        self.labels = json.load(open(json_file, 'r', encoding='utf-8'))\n",
        "        self.coco_id_name_map = self._categories()\n",
        "        self.coco_name_list = list(self.coco_id_name_map.values())\n",
        "        self.output = output\n",
        "        print(\"total images\", len(self.labels['images']))\n",
        "        print(\"total categories\", len(self.labels['categories']))\n",
        "        print(\"total labels\", len(self.labels['annotations']))\n",
        "\n",
        "    def _check_file_and_dir(self, file_path, dir_path):\n",
        "        if not os.path.exists(file_path):\n",
        "            raise ValueError(\"file not found\")\n",
        "        if not os.path.exists(dir_path):\n",
        "            os.makedirs(dir_path)\n",
        "\n",
        "    def _categories(self):\n",
        "        categories = {}\n",
        "        for cls in self.labels['categories']:\n",
        "            categories[cls['id']] = cls['name']\n",
        "        return categories\n",
        "\n",
        "    def _load_images_info(self):\n",
        "        images_info = {}\n",
        "        for image in self.labels['images']:\n",
        "            id = image['id']\n",
        "            file_name = image['file_name']\n",
        "            if file_name.find('\\\\') > -1:\n",
        "                file_name = file_name[file_name.index('\\\\')+1:]\n",
        "            w = image['width']\n",
        "            h = image['height']\n",
        "            images_info[id] = (file_name, w, h)\n",
        "\n",
        "        return images_info\n",
        "\n",
        "    def _bbox_2_yolo(self, bbox, img_w, img_h):\n",
        "        x, y, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "        centerx = bbox[0] + w / 2\n",
        "        centery = bbox[1] + h / 2\n",
        "        dw = 1 / img_w\n",
        "        dh = 1 / img_h\n",
        "        centerx *= dw\n",
        "        w *= dw\n",
        "        centery *= dh\n",
        "        h *= dh\n",
        "        return centerx, centery, w, h\n",
        "\n",
        "    def _convert_anno(self, images_info):\n",
        "        anno_dict = dict()\n",
        "        for anno in self.labels['annotations']:\n",
        "            bbox = anno['bbox']\n",
        "            image_id = anno['image_id']\n",
        "            category_id = anno['category_id']\n",
        "\n",
        "            image_info = images_info.get(image_id)\n",
        "            image_name = image_info[0]\n",
        "            img_w = image_info[1]\n",
        "            img_h = image_info[2]\n",
        "            yolo_box = self._bbox_2_yolo(bbox, img_w, img_h)\n",
        "\n",
        "            anno_info = (image_name, category_id, yolo_box)\n",
        "            anno_infos = anno_dict.get(image_id)\n",
        "            if not anno_infos:\n",
        "                anno_dict[image_id] = [anno_info]\n",
        "            else:\n",
        "                anno_infos.append(anno_info)\n",
        "                anno_dict[image_id] = anno_infos\n",
        "        return anno_dict\n",
        "\n",
        "    def save_classes(self):\n",
        "        sorted_classes = list(map(lambda x: x['name'], sorted(self.labels['categories'], key=lambda x: x['id'])))\n",
        "        print('coco names', sorted_classes)\n",
        "        with open('coco.names', 'w', encoding='utf-8') as f:\n",
        "            for cls in sorted_classes:\n",
        "                f.write(cls + '\\n')\n",
        "        f.close()\n",
        "\n",
        "    def coco2yolo(self):\n",
        "        print(\"loading image info...\")\n",
        "        images_info = self._load_images_info()\n",
        "        print(\"loading done, total images\", len(images_info))\n",
        "\n",
        "        print(\"start converting...\")\n",
        "        anno_dict = self._convert_anno(images_info)\n",
        "        print(\"converting done, total labels\", len(anno_dict))\n",
        "\n",
        "        print(\"saving txt file...\")\n",
        "        self._save_txt(anno_dict)\n",
        "        print(\"saving done\")\n",
        "\n",
        "    def _save_txt(self, anno_dict):\n",
        "        for k, v in anno_dict.items():\n",
        "            file_name = os.path.splitext(v[0][0])[0] + \".txt\"\n",
        "            with open(os.path.join(self.output, file_name), 'w', encoding='utf-8') as f:\n",
        "                print(k, v)\n",
        "                for obj in v:\n",
        "                    cat_name = self.coco_id_name_map.get(obj[1])\n",
        "                    category_id = self.coco_name_list.index(cat_name)\n",
        "                    box = ['{:.6f}'.format(x) for x in obj[2]]\n",
        "                    box = ' '.join(box)\n",
        "                    line = str(category_id) + ' ' + box\n",
        "                    f.write(line + '\\n')\n",
        "                    if category_id not in count_dict:\n",
        "                        count_dict[category_id] = 1\n",
        "                    else:\n",
        "                        count_dict[category_id] += 1\n",
        "        print(count_dict)"
      ],
      "metadata": {
        "id": "5Fq9PRlOvSuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_root_path = '/content/drive/MyDrive/SEM 1/TML/data/vehicles.v2-release.coco/annotations'\n",
        "save_root_path = '/content/drive/MyDrive/SEM 1/TML/data/yolo_data/labels'\n",
        "## For training data\n",
        "dtype = 'train'\n",
        "json_file = os.path.join(json_root_path, dtype + '_annotations_coco.json')\n",
        "save_path = os.path.join(save_root_path, dtype)\n",
        "c2y = COCO2YOLO(json_file, save_path)\n",
        "c2y.coco2yolo()\n",
        "\n",
        "## For validation data\n",
        "dtype = 'val'\n",
        "json_file = os.path.join(json_root_path, dtype + '_annotations_coco.json')\n",
        "save_path = os.path.join(save_root_path, dtype)\n",
        "c2y = COCO2YOLO(json_file, save_path)\n",
        "c2y.coco2yolo()\n",
        "\n",
        "## For testing data\n",
        "dtype = 'test'\n",
        "json_file = os.path.join(json_root_path, dtype + '_annotations_coco.json')\n",
        "save_path = os.path.join(save_root_path, dtype)\n",
        "c2y = COCO2YOLO(json_file, save_path)\n",
        "c2y.coco2yolo()"
      ],
      "metadata": {
        "id": "edxJ8ja0v5vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making config files\n",
        "\n",
        "#### Storing the below config in data.yaml file, which will be passed to training module"
      ],
      "metadata": {
        "id": "bfsI0dcHuzmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train: /content/drive/MyDrive/SEM 1/TML/data/yolo_data/images/train/\n",
        "val:  /content/drive/MyDrive/SEM 1/TML/data/yolo_data/images/val/\n",
        "test: /content/drive/MyDrive/SEM 1/TML/data/yolo_data/images/test\n",
        "\n",
        "# number of classes\n",
        "nc: 13\n",
        "\n",
        "# class names\n",
        "names:\n",
        "  0: vehicles\n",
        "  1: big bus\n",
        "  2: big truck\n",
        "  3: bus-l-\n",
        "  4: bus-s-\n",
        "  5: car\n",
        "  6: mid truck\n",
        "  7: small bus\n",
        "  8: small truck\n",
        "  9: truck-l-\n",
        "  10: truck-m-\n",
        "  11: truck-s-\n",
        "  12: truck-xl-"
      ],
      "metadata": {
        "id": "AUcuOfWWusMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training yolov5 small on default parameters\n",
        "### Arguments to be passed -\n",
        "* img - Size in which images are to be resized according to resolution requirement\n",
        "* batch - Batch size\n",
        "* epochs - Number of epochs\n",
        "* data - Path to data config file\n",
        "* weights - Path to pre-trained model"
      ],
      "metadata": {
        "id": "rh_rCp1WxtXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 32 --epochs 300 --data data/data.yaml --weights \"/content/drive/MyDrive/SEM 1/TML/yolov5/weights/yolov5s.pt\""
      ],
      "metadata": {
        "id": "Y8UsCoY1vDOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training yolov5 small on with freezing the backbone architecture"
      ],
      "metadata": {
        "id": "zS12d3A0yc3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 32 --epochs 100 --data data/data.yaml --weights \"/content/drive/MyDrive/SEM 1/TML/yolov5/weights/yolov5s.pt\" --cfg models/yolov5n.yaml --freeze 10"
      ],
      "metadata": {
        "id": "iWkgJw3TycRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training yolov5 small on with freezing all the layers"
      ],
      "metadata": {
        "id": "sYep5bU2ylxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 32 --epochs 100 --data data/data.yaml --weights \"/content/drive/MyDrive/SEM 1/TML/yolov5/weights/yolov5s.pt\" --cfg models/yolov5n.yaml --freeze 24"
      ],
      "metadata": {
        "id": "KkemSEHIyqxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation on Testing Data\n",
        "\n",
        "### Arguments to be passed -\n",
        "* img - Size in which images are to be resized according to resolution requirement\n",
        "* data - Path to data config file\n",
        "* weights - Path to the best saved model from training"
      ],
      "metadata": {
        "id": "LIpS2K0tyxEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights \"/content/drive/MyDrive/SEM 1/TML/yolov5/runs/train/exp7_90epochs/weights/best.pt\" --data data/data.yaml --img 640"
      ],
      "metadata": {
        "id": "xuXlSO7ny3wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference on Single image\n",
        "\n",
        "### Arguments to be passed -\n",
        "* img - Size in which images are to be resized according to resolution requirement\n",
        "* data - Path to data config file\n",
        "* weights - Path to the best saved model from training\n",
        "* conf - to set confidence threshold\n",
        "* source - Path to image\n",
        "* line-thichness = to make small boundaries while plotting the predictions"
      ],
      "metadata": {
        "id": "wRbBirdRzDbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights \"/content/drive/MyDrive/SEM 1/TML/yolov5/runs/train/exp3_30epochs_yolos/weights/best.pt\" --img 640 --conf 0.25 --source \"/content/drive/MyDrive/SEM 1/TML/data/yolo_data/images/test/adit_mp4-1002_jpg.rf.5e4018e963af1251b3f7e6fd487c479e.jpg\" --line-thickness 1"
      ],
      "metadata": {
        "id": "_bgySWhtzGSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}